{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1be6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCopyright (c) 2025 NeuralSense AI Private Limited\\nTrading as swatah.ai. All rights reserved.\\nThis file is part of the swatah.ai software stack and is licensed under\\nthe terms defined in the accompanying LICENSE file. Unauthorized copying,\\ndistribution, or modification of this file, via any medium, is strictly prohibited.\\nFor more information, visit: https://swatah.ai\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright (c) 2025 NeuralSense AI Private Limited\n",
    "Trading as swatah.ai. All rights reserved.\n",
    "This file is part of the swatah.ai software stack and is licensed under\n",
    "the terms defined in the accompanying LICENSE file. Unauthorized copying,\n",
    "distribution, or modification of this file, via any medium, is strictly prohibited.\n",
    "For more information, visit: https://swatah.ai\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541b7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/radxa/dev/mobilenetv3\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e7a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rknnlite.api import RKNNLite\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8796c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    image = image.resize((256, 256), Image.BILINEAR)\n",
    "    left = (256 - 224) // 2\n",
    "    top = (256 - 224) // 2\n",
    "    image = image.crop((left, top, left + 224, top + 224))\n",
    "\n",
    "    img = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "    # mean = np.array([0.485, 0.456, 0.406])\n",
    "    # std = np.array([0.229, 0.224, 0.225])\n",
    "    # img = (img - mean) / std\n",
    "\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d21cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W rknn-toolkit-lite2 version: 2.3.0\n",
      "W Query dynamic range failed. Ret code: RKNN_ERR_MODEL_INVALID. (If it is a static shape RKNN model, please ignore the above warning message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I RKNN: [23:38:19.098] RKNN Runtime Information, librknnrt version: 2.3.0 (c949ad889d@2024-11-07T11:35:33)\n",
      "I RKNN: [23:38:19.098] RKNN Driver Information, version: 0.9.8\n",
      "I RKNN: [23:38:19.098] RKNN Model Information, version: 6, toolkit version: 2.3.2(compiler version: 2.3.2 (@2025-04-03T08:26:16)), target: RKNPU v2, target platform: rk3588, framework name: ONNX, framework layout: NCHW, model inference type: static_shape\n",
      "W RKNN: [23:38:19.118] query RKNN_QUERY_INPUT_DYNAMIC_RANGE error, rknn model is static shape type, please export rknn with dynamic_shapes\n"
     ]
    }
   ],
   "source": [
    "rknn = RKNNLite()\n",
    "rknn.load_rknn('models/mobilenetv3_large.rknn')\n",
    "rknn.init_runtime()\n",
    "\n",
    "img_path = \"images/hen.jpg\"\n",
    "img = preprocess_image(img_path)\n",
    "\n",
    "outputs = rknn.inference(inputs=[img])\n",
    "\n",
    "rknn.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47bb15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 7\n",
      "Predicted class name: cock\n"
     ]
    }
   ],
   "source": [
    "predicted_class = np.argmax(outputs[0])\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "class_names = urllib.request.urlopen(LABELS_URL).read().decode(\"utf-8\").split(\"\\n\")\n",
    "\n",
    "print(f\"Predicted class name: {class_names[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564638c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
